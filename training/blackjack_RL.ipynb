{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-22T13:29:33.577798Z",
     "start_time": "2026-01-22T13:29:33.566861Z"
    }
   },
   "source": "from AI.Blackjack_RL_project.environment.blackjack import Blackjack",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T17:09:28.994689Z",
     "start_time": "2026-01-21T17:09:28.992665Z"
    }
   },
   "cell_type": "code",
   "source": "env = Blackjack()",
   "id": "696c52c430375d02",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T17:09:28.998533Z",
     "start_time": "2026-01-21T17:09:28.997091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "id": "3f6b8dd3c7db50ea",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T17:09:29.004258Z",
     "start_time": "2026-01-21T17:09:29.002588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.accelerator.current_accelerator() if torch.accelerator.is_available else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(f\"device: {device}\")"
   ],
   "id": "ace024bb36fcd00c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T17:09:29.021588Z",
     "start_time": "2026-01-21T17:09:29.018991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = x.clone()\n",
    "        x_norm[:, 0] = x[:, 0] / 21.0\n",
    "        x_norm[:, 1] = x[:, 1] / 11.0\n",
    "        logits = self.layers(x_norm)\n",
    "        return logits"
   ],
   "id": "68889bc710400c17",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T17:09:29.031751Z",
     "start_time": "2026-01-21T17:09:29.028986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(11)\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ],
   "id": "c127cfbb37e7a3db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T17:09:29.040996Z",
     "start_time": "2026-01-21T17:09:29.038811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from collections import deque\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ],
   "id": "f4d285205b92e8ee",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T17:09:29.096869Z",
     "start_time": "2026-01-21T17:09:29.051097Z"
    }
   },
   "cell_type": "code",
   "source": "buffer = ReplayBuffer(100000)",
   "id": "49ef4fdbda778ede",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T17:09:29.101115Z",
     "start_time": "2026-01-21T17:09:29.099391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_action(state, epsilon, double_possible, splittable):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_random(double_possible, splittable)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state[:, :3])\n",
    "        if not double_possible:\n",
    "            q_values[:, 2] = -99.0\n",
    "        if not splittable:\n",
    "            q_values[:, 3] = -99.0\n",
    "\n",
    "    return torch.argmax(q_values).item()"
   ],
   "id": "921fed80f900488",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T17:09:29.105346Z",
     "start_time": "2026-01-21T17:09:29.103100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "def training(target_model):\n",
    "    batch = zip(*buffer.sample(batch_size))\n",
    "\n",
    "    state, action, reward, next_state, done = batch\n",
    "\n",
    "    state = torch.stack(state).to(device).squeeze(1)\n",
    "    action = torch.tensor(action).long().unsqueeze(1).to(device)\n",
    "    reward = torch.tensor(reward).float().unsqueeze(1).to(device)\n",
    "    next_state = torch.stack(next_state).to(device).squeeze(1)\n",
    "    done = torch.tensor(done).float().unsqueeze(1).to(device)\n",
    "\n",
    "    output = model(state[:, :3])\n",
    "    label = output.gather(dim=1, index=action)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        target_q_vals = target_model(next_state[:, :3])\n",
    "\n",
    "        action_split = (action[:] == 3) + 1.0\n",
    "        target_q_vals *= action_split\n",
    "\n",
    "        cannot_double = (next_state[:, -2] == 0)\n",
    "        cannot_split = (next_state[:, -1] == 0)\n",
    "\n",
    "        target_q_vals[cannot_double, 2] = -99.0\n",
    "        target_q_vals[cannot_split, 3] = -99.0\n",
    "        future_move = torch.amax(target_q_vals, dim=1).unsqueeze(1)\n",
    "        prediction = reward + (1 - done) * gamma * future_move\n",
    "    loss = F.smooth_l1_loss(label, prediction)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ],
   "id": "dd1d75a74e902968",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T17:26:22.695975Z",
     "start_time": "2026-01-21T17:09:29.108203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "epochs = 1000000\n",
    "batch_size = 256\n",
    "gamma = 1.0\n",
    "epsilon = 1.0\n",
    "target_model = copy.deepcopy(model)\n",
    "model.train()\n",
    "reward_sum = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    state = env.next_hand()\n",
    "\n",
    "    state = torch.tensor(state, device=device).float().unsqueeze(0)\n",
    "    done = False\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    while not done:\n",
    "        double_possible = state[:, -2]\n",
    "        splittable = state[:, -1]\n",
    "\n",
    "        if buffer.size() < 10 * batch_size:\n",
    "            action = env.action_random(double_possible, splittable)\n",
    "        else:\n",
    "            action = get_action(state, epsilon, double_possible, splittable)\n",
    "\n",
    "        next_state, reward, hand_over = env.step(action)\n",
    "        if len(next_state) == 2:\n",
    "            next_state = next_state[0]\n",
    "        next_state = torch.tensor(next_state, device=device).float().unsqueeze(0)\n",
    "\n",
    "        done = hand_over or action == 3\n",
    "\n",
    "        buffer.push(state, action, reward, next_state, hand_over)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    reward_sum += reward\n",
    "\n",
    "    if buffer.size() < 10 * batch_size:\n",
    "        continue\n",
    "\n",
    "    loss = training(target_model)\n",
    "\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f\"epoch: {epoch} | reward net: {round(reward_sum)}\")\n",
    "        reward_sum = 0\n",
    "\n",
    "    if epoch < 0.8 * epochs:\n",
    "        epsilon = max(0.1, epsilon*0.99999)\n",
    "    else:\n",
    "        epsilon = 0\n"
   ],
   "id": "c29abad39fbf51b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10000 | reward net: -4727\n",
      "epoch: 20000 | reward net: -4445\n",
      "epoch: 30000 | reward net: -3987\n",
      "epoch: 40000 | reward net: -3966\n",
      "epoch: 50000 | reward net: -3314\n",
      "epoch: 60000 | reward net: -3086\n",
      "epoch: 70000 | reward net: -2913\n",
      "epoch: 80000 | reward net: -2544\n",
      "epoch: 90000 | reward net: -2650\n",
      "epoch: 100000 | reward net: -2119\n",
      "epoch: 110000 | reward net: -1951\n",
      "epoch: 120000 | reward net: -1891\n",
      "epoch: 130000 | reward net: -1887\n",
      "epoch: 140000 | reward net: -1681\n",
      "epoch: 150000 | reward net: -1437\n",
      "epoch: 160000 | reward net: -1522\n",
      "epoch: 170000 | reward net: -1469\n",
      "epoch: 180000 | reward net: -1356\n",
      "epoch: 190000 | reward net: -1160\n",
      "epoch: 200000 | reward net: -1031\n",
      "epoch: 210000 | reward net: -911\n",
      "epoch: 220000 | reward net: -1240\n",
      "epoch: 230000 | reward net: -955\n",
      "epoch: 240000 | reward net: -906\n",
      "epoch: 250000 | reward net: -1067\n",
      "epoch: 260000 | reward net: -879\n",
      "epoch: 270000 | reward net: -1051\n",
      "epoch: 280000 | reward net: -868\n",
      "epoch: 290000 | reward net: -1192\n",
      "epoch: 300000 | reward net: -938\n",
      "epoch: 310000 | reward net: -859\n",
      "epoch: 320000 | reward net: -960\n",
      "epoch: 330000 | reward net: -954\n",
      "epoch: 340000 | reward net: -1177\n",
      "epoch: 350000 | reward net: -1011\n",
      "epoch: 360000 | reward net: -828\n",
      "epoch: 370000 | reward net: -971\n",
      "epoch: 380000 | reward net: -874\n",
      "epoch: 390000 | reward net: -887\n",
      "epoch: 400000 | reward net: -902\n",
      "epoch: 410000 | reward net: -757\n",
      "epoch: 420000 | reward net: -881\n",
      "epoch: 430000 | reward net: -1010\n",
      "epoch: 440000 | reward net: -961\n",
      "epoch: 450000 | reward net: -654\n",
      "epoch: 460000 | reward net: -687\n",
      "epoch: 470000 | reward net: -1142\n",
      "epoch: 480000 | reward net: -943\n",
      "epoch: 490000 | reward net: -1022\n",
      "epoch: 500000 | reward net: -772\n",
      "epoch: 510000 | reward net: -796\n",
      "epoch: 520000 | reward net: -915\n",
      "epoch: 530000 | reward net: -965\n",
      "epoch: 540000 | reward net: -864\n",
      "epoch: 550000 | reward net: -1029\n",
      "epoch: 560000 | reward net: -870\n",
      "epoch: 570000 | reward net: -972\n",
      "epoch: 580000 | reward net: -721\n",
      "epoch: 590000 | reward net: -738\n",
      "epoch: 600000 | reward net: -1032\n",
      "epoch: 610000 | reward net: -980\n",
      "epoch: 620000 | reward net: -880\n",
      "epoch: 630000 | reward net: -968\n",
      "epoch: 640000 | reward net: -922\n",
      "epoch: 650000 | reward net: -890\n",
      "epoch: 660000 | reward net: -801\n",
      "epoch: 670000 | reward net: -535\n",
      "epoch: 680000 | reward net: -819\n",
      "epoch: 690000 | reward net: -775\n",
      "epoch: 700000 | reward net: -690\n",
      "epoch: 710000 | reward net: -861\n",
      "epoch: 720000 | reward net: -878\n",
      "epoch: 730000 | reward net: -750\n",
      "epoch: 740000 | reward net: -944\n",
      "epoch: 750000 | reward net: -964\n",
      "epoch: 760000 | reward net: -975\n",
      "epoch: 770000 | reward net: -786\n",
      "epoch: 780000 | reward net: -971\n",
      "epoch: 790000 | reward net: -1106\n",
      "epoch: 800000 | reward net: -887\n",
      "epoch: 810000 | reward net: -369\n",
      "epoch: 820000 | reward net: -592\n",
      "epoch: 830000 | reward net: -386\n",
      "epoch: 840000 | reward net: -498\n",
      "epoch: 850000 | reward net: -427\n",
      "epoch: 860000 | reward net: -564\n",
      "epoch: 870000 | reward net: -510\n",
      "epoch: 880000 | reward net: -459\n",
      "epoch: 890000 | reward net: -463\n",
      "epoch: 900000 | reward net: -598\n",
      "epoch: 910000 | reward net: -497\n",
      "epoch: 920000 | reward net: -497\n",
      "epoch: 930000 | reward net: -424\n",
      "epoch: 940000 | reward net: -405\n",
      "epoch: 950000 | reward net: -484\n",
      "epoch: 960000 | reward net: -485\n",
      "epoch: 970000 | reward net: -450\n",
      "epoch: 980000 | reward net: -438\n",
      "epoch: 990000 | reward net: -270\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T13:38:57.413133Z",
     "start_time": "2026-01-22T13:38:57.403943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state = env.next_hand()\n",
    "state = (11, 5, 0)\n",
    "print(state)\n",
    "state = torch.tensor(state, device=device).float().unsqueeze(0)\n",
    "output = model(state)\n",
    "print(output)\n",
    "action = torch.argmax(output).item()"
   ],
   "id": "177abaf744ed591f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 5, 0)\n",
      "tensor([[-0.1143,  0.3851,  1.5147,  0.3546]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:15:43.271876Z",
     "start_time": "2026-01-21T18:15:36.200797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "games = 100000\n",
    "model.eval()\n",
    "wins = 0\n",
    "draws = 0\n",
    "losses = 0\n",
    "net_score = 0\n",
    "for game in range(games):\n",
    "    state = env.next_hand()\n",
    "    state = torch.tensor(state, device=device).float().unsqueeze(0)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state[:, :3])\n",
    "            if state[0, -2] == 0:\n",
    "                q_values[0, -2] = -99\n",
    "            if state[0, -1] == 0:\n",
    "                q_values[0, -1] = -99\n",
    "\n",
    "            action = torch.argmax(q_values).item()\n",
    "\n",
    "        if action == 3:\n",
    "            _ = env.step(action)\n",
    "            next_state = env.next_hand()\n",
    "            terminated = False\n",
    "\n",
    "        else:\n",
    "            next_state, reward, terminated = env.step(action)\n",
    "\n",
    "        next_state = torch.tensor(next_state, device=device).float().unsqueeze(0)\n",
    "\n",
    "        done = terminated\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    if reward > 0:\n",
    "        wins += 1\n",
    "    elif reward == 0:\n",
    "        draws += 1\n",
    "    else:\n",
    "        losses += 1\n",
    "\n",
    "    net_score += reward\n",
    "\n",
    "print(f\"Wins: {wins} | Draws: {draws} | Losses: {losses}\")\n",
    "print(f\"Win Accuracy: {(wins/games)*100:.2f}% | Draw Accuracy: {(draws/games)*100:.2f}% | Loss Accuracy: {(losses/games)*100:.2f}%\")\n",
    "print(f\"Net Score: {net_score}\")"
   ],
   "id": "50e09d818b7e1286",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins: 41975 | Draws: 9334 | Losses: 48691\n",
      "Win Accuracy: 41.98% | Draw Accuracy: 9.33% | Loss Accuracy: 48.69%\n",
      "Net Score: -3578.0\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T18:30:03.684430Z",
     "start_time": "2026-01-21T18:30:03.678279Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), \"blackjack_model_weights.pth\")",
   "id": "f7209e2808724c79",
   "outputs": [],
   "execution_count": 73
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
